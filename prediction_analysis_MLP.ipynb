{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba93c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import timedelta\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# Use sklearn to split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import statsmodels.formula.api as smf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758de6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>property_id</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>building_year_built</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>Address_Google</th>\n",
       "      <th>closest_zip_1</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>closest_zip_2</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Unemployment Rate_3</th>\n",
       "      <th>sale_price_1</th>\n",
       "      <th>sale_price_2</th>\n",
       "      <th>sale_price_3</th>\n",
       "      <th>California AGI_1</th>\n",
       "      <th>California AGI_2</th>\n",
       "      <th>California AGI_3</th>\n",
       "      <th>Labor Force_1</th>\n",
       "      <th>Labor Force_2</th>\n",
       "      <th>Labor Force_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2506034039</td>\n",
       "      <td>240002</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12626 SAN FERNANDO RD, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2505017041</td>\n",
       "      <td>331000</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12938 DE HAVEN AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2530002019</td>\n",
       "      <td>299000</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>11521 HELA AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2506029006</td>\n",
       "      <td>200002</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12765 WOODCOCK AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2506035025</td>\n",
       "      <td>275002</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12577 RALSTON AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  property_id  sale_price  building_year_built  zip_code  \\\n",
       "0           0   2506034039      240002               1943.0     91342   \n",
       "1           1   2505017041      331000               1965.0     91342   \n",
       "2           2   2530002019      299000               1958.0     91342   \n",
       "3           3   2506029006      200002               1948.0     91342   \n",
       "4           4   2506035025      275002               1947.0     91342   \n",
       "\n",
       "                                      Address_Google  closest_zip_1  \\\n",
       "0  12626 SAN FERNANDO RD, LOS ANGELES, CA 91342, USA          91340   \n",
       "1     12938 DE HAVEN AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "2         11521 HELA AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "3     12765 WOODCOCK AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "4      12577 RALSTON AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "\n",
       "   distance_1  closest_zip_2  distance_2  ...  Unemployment Rate_3  \\\n",
       "0        1.58          91344        2.94  ...                  0.0   \n",
       "1        1.58          91344        2.94  ...                  0.0   \n",
       "2        1.58          91344        2.94  ...                  0.0   \n",
       "3        1.58          91344        2.94  ...                  0.0   \n",
       "4        1.58          91344        2.94  ...                  0.0   \n",
       "\n",
       "   sale_price_1  sale_price_2  sale_price_3  California AGI_1  \\\n",
       "0           0.0           0.0           0.0               0.0   \n",
       "1           0.0           0.0           0.0               0.0   \n",
       "2           0.0           0.0           0.0               0.0   \n",
       "3           0.0           0.0           0.0               0.0   \n",
       "4           0.0           0.0           0.0               0.0   \n",
       "\n",
       "   California AGI_2 California AGI_3  Labor Force_1  Labor Force_2  \\\n",
       "0               0.0              0.0            0.0            0.0   \n",
       "1               0.0              0.0            0.0            0.0   \n",
       "2               0.0              0.0            0.0            0.0   \n",
       "3               0.0              0.0            0.0            0.0   \n",
       "4               0.0              0.0            0.0            0.0   \n",
       "\n",
       "  Labor Force_3  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('final_df_processed_3.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6260f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937473"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f3823f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327064"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df.loc[data_df[\"Year\"] >= 2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34cb0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>property_id</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>building_year_built</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>Address_Google</th>\n",
       "      <th>closest_zip_1</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>closest_zip_2</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>...</th>\n",
       "      <th>Unemployment Rate_3</th>\n",
       "      <th>sale_price_1</th>\n",
       "      <th>sale_price_2</th>\n",
       "      <th>sale_price_3</th>\n",
       "      <th>California AGI_1</th>\n",
       "      <th>California AGI_2</th>\n",
       "      <th>California AGI_3</th>\n",
       "      <th>Labor Force_1</th>\n",
       "      <th>Labor Force_2</th>\n",
       "      <th>Labor Force_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>10003</td>\n",
       "      <td>3146014003</td>\n",
       "      <td>100000</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>44533 HANSTEAD AVE, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>10004</td>\n",
       "      <td>3140023050</td>\n",
       "      <td>175001</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>43711 RAYSACK AVE, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>10005</td>\n",
       "      <td>3170044036</td>\n",
       "      <td>118000</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>3004 HILDRETH CT, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>10006</td>\n",
       "      <td>3150026080</td>\n",
       "      <td>184501</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>2220 E NUGENT ST, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>10007</td>\n",
       "      <td>3148018075</td>\n",
       "      <td>100001</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>1344 FRANKLIN AVE, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21860</th>\n",
       "      <td>21860</td>\n",
       "      <td>3162010029</td>\n",
       "      <td>345003</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>17110 E AVENUE K12, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.531513</td>\n",
       "      <td>0.403551</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.161483</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.003239</td>\n",
       "      <td>0.508263</td>\n",
       "      <td>-0.591260</td>\n",
       "      <td>0.719562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21861</th>\n",
       "      <td>21861</td>\n",
       "      <td>3070006008</td>\n",
       "      <td>325003</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>41028 168TH ST E, LAKE LOS ANGELES, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524676</td>\n",
       "      <td>0.398840</td>\n",
       "      <td>0.713835</td>\n",
       "      <td>0.159818</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>0.501847</td>\n",
       "      <td>-0.583261</td>\n",
       "      <td>0.708633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21862</th>\n",
       "      <td>21862</td>\n",
       "      <td>3176031036</td>\n",
       "      <td>566005</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>1748 E HOLGUIN ST, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.517964</td>\n",
       "      <td>0.394198</td>\n",
       "      <td>0.703051</td>\n",
       "      <td>0.158170</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.495544</td>\n",
       "      <td>-0.575425</td>\n",
       "      <td>0.697979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21863</th>\n",
       "      <td>21863</td>\n",
       "      <td>3170040083</td>\n",
       "      <td>435004</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>1034 E AVENUE K11, LANCASTER, CA 93535, USA</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511374</td>\n",
       "      <td>0.389625</td>\n",
       "      <td>0.692535</td>\n",
       "      <td>0.156541</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.489351</td>\n",
       "      <td>-0.567746</td>\n",
       "      <td>0.687588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21864</th>\n",
       "      <td>21864</td>\n",
       "      <td>3069013006</td>\n",
       "      <td>243502</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>93535</td>\n",
       "      <td>15622 VALEPORT AVE, LAKE LOS ANGELES, CA 93535...</td>\n",
       "      <td>93534</td>\n",
       "      <td>2.22</td>\n",
       "      <td>93536</td>\n",
       "      <td>5.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504903</td>\n",
       "      <td>0.385119</td>\n",
       "      <td>0.682277</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.483265</td>\n",
       "      <td>-0.560219</td>\n",
       "      <td>0.677450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11862 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  property_id  sale_price  building_year_built  zip_code  \\\n",
       "10003       10003   3146014003      100000               1964.0     93535   \n",
       "10004       10004   3140023050      175001               2007.0     93535   \n",
       "10005       10005   3170044036      118000               1992.0     93535   \n",
       "10006       10006   3150026080      184501               2009.0     93535   \n",
       "10007       10007   3148018075      100001               1980.0     93535   \n",
       "...           ...          ...         ...                  ...       ...   \n",
       "21860       21860   3162010029      345003               1956.0     93535   \n",
       "21861       21861   3070006008      325003               1987.0     93535   \n",
       "21862       21862   3176031036      566005               2019.0     93535   \n",
       "21863       21863   3170040083      435004               2004.0     93535   \n",
       "21864       21864   3069013006      243502               1985.0     93535   \n",
       "\n",
       "                                          Address_Google  closest_zip_1  \\\n",
       "10003       44533 HANSTEAD AVE, LANCASTER, CA 93535, USA          93534   \n",
       "10004        43711 RAYSACK AVE, LANCASTER, CA 93535, USA          93534   \n",
       "10005         3004 HILDRETH CT, LANCASTER, CA 93535, USA          93534   \n",
       "10006         2220 E NUGENT ST, LANCASTER, CA 93535, USA          93534   \n",
       "10007        1344 FRANKLIN AVE, LANCASTER, CA 93535, USA          93534   \n",
       "...                                                  ...            ...   \n",
       "21860       17110 E AVENUE K12, LANCASTER, CA 93535, USA          93534   \n",
       "21861  41028 168TH ST E, LAKE LOS ANGELES, CA 93535, USA          93534   \n",
       "21862        1748 E HOLGUIN ST, LANCASTER, CA 93535, USA          93534   \n",
       "21863        1034 E AVENUE K11, LANCASTER, CA 93535, USA          93534   \n",
       "21864  15622 VALEPORT AVE, LAKE LOS ANGELES, CA 93535...          93534   \n",
       "\n",
       "       distance_1  closest_zip_2  distance_2  ...  Unemployment Rate_3  \\\n",
       "10003        2.22          93536        5.13  ...             0.000000   \n",
       "10004        2.22          93536        5.13  ...             0.000000   \n",
       "10005        2.22          93536        5.13  ...             0.000000   \n",
       "10006        2.22          93536        5.13  ...             0.000000   \n",
       "10007        2.22          93536        5.13  ...             0.000000   \n",
       "...           ...            ...         ...  ...                  ...   \n",
       "21860        2.22          93536        5.13  ...            -0.531513   \n",
       "21861        2.22          93536        5.13  ...            -0.524676   \n",
       "21862        2.22          93536        5.13  ...            -0.517964   \n",
       "21863        2.22          93536        5.13  ...            -0.511374   \n",
       "21864        2.22          93536        5.13  ...            -0.504903   \n",
       "\n",
       "       sale_price_1  sale_price_2  sale_price_3  California AGI_1  \\\n",
       "10003      0.000000      0.000000      0.000000          0.000000   \n",
       "10004      0.000000      0.000000      0.000000          0.000000   \n",
       "10005      0.000000      0.000000      0.000000          0.000000   \n",
       "10006      0.000000      0.000000      0.000000          0.000000   \n",
       "10007      0.000000      0.000000      0.000000          0.000000   \n",
       "...             ...           ...           ...               ...   \n",
       "21860      0.403551      0.724900      0.161483          0.000599   \n",
       "21861      0.398840      0.713835      0.159818          0.000593   \n",
       "21862      0.394198      0.703051      0.158170          0.000587   \n",
       "21863      0.389625      0.692535      0.156541          0.000582   \n",
       "21864      0.385119      0.682277      0.154930          0.000576   \n",
       "\n",
       "       California AGI_2 California AGI_3  Labor Force_1  Labor Force_2  \\\n",
       "10003          0.000000         0.000000       0.000000       0.000000   \n",
       "10004          0.000000         0.000000       0.000000       0.000000   \n",
       "10005          0.000000         0.000000       0.000000       0.000000   \n",
       "10006          0.000000         0.000000       0.000000       0.000000   \n",
       "10007          0.000000         0.000000       0.000000       0.000000   \n",
       "...                 ...              ...            ...            ...   \n",
       "21860          0.001549         0.003239       0.508263      -0.591260   \n",
       "21861          0.001534         0.003206       0.501847      -0.583261   \n",
       "21862          0.001518         0.003174       0.495544      -0.575425   \n",
       "21863          0.001503         0.003142       0.489351      -0.567746   \n",
       "21864          0.001488         0.003111       0.483265      -0.560219   \n",
       "\n",
       "      Labor Force_3  \n",
       "10003      0.000000  \n",
       "10004      0.000000  \n",
       "10005      0.000000  \n",
       "10006      0.000000  \n",
       "10007      0.000000  \n",
       "...             ...  \n",
       "21860      0.719562  \n",
       "21861      0.708633  \n",
       "21862      0.697979  \n",
       "21863      0.687588  \n",
       "21864      0.677450  \n",
       "\n",
       "[11862 rows x 71 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_df = data_df.loc[data_df['zip_code'] == 93535]\n",
    "zip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1810f951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x30ecebd00>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(zip_df.index, zip_df['sale_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5ca039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:      change_sale_price   R-squared:                       0.076\n",
      "Model:                            OLS   Adj. R-squared:                  0.076\n",
      "Method:                 Least Squares   F-statistic:                     974.4\n",
      "Date:                Sun, 23 Feb 2025   Prob (F-statistic):          1.20e-205\n",
      "Time:                        20:19:19   Log-Likelihood:            -1.6774e+05\n",
      "No. Observations:               11861   AIC:                         3.355e+05\n",
      "Df Residuals:                   11859   BIC:                         3.355e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept        4.921e+04   3459.152     14.226      0.000    4.24e+04     5.6e+04\n",
      "sale_price_Prev    -0.1519      0.005    -31.216      0.000      -0.161      -0.142\n",
      "==============================================================================\n",
      "Omnibus:                     9918.412   Durbin-Watson:                   2.733\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         16832668.430\n",
      "Skew:                           2.649   Prob(JB):                         0.00\n",
      "Kurtosis:                     187.477   Cond. No.                     7.99e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.99e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g3/164q6cz52jv0sjllnsg7jdsm0000gn/T/ipykernel_7957/2461574441.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  zip_df['sale_price_Prev'] = zip_df['sale_price'].shift(1)\n",
      "/var/folders/g3/164q6cz52jv0sjllnsg7jdsm0000gn/T/ipykernel_7957/2461574441.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  zip_df['change_sale_price'] = zip_df['sale_price'] - zip_df['sale_price_Prev']\n"
     ]
    }
   ],
   "source": [
    "zip_df['sale_price_Prev'] = zip_df['sale_price'].shift(1)\n",
    "zip_df['change_sale_price'] = zip_df['sale_price'] - zip_df['sale_price_Prev']\n",
    "\n",
    "\n",
    "mod1 = smf.ols(f'change_sale_price ~ sale_price_Prev', data = zip_df).fit() \n",
    "print(mod1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9497e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-stat for sale_price_Prev: -31.21554567888494\n"
     ]
    }
   ],
   "source": [
    "# Extract the t-statistic for sale_price_Prev\n",
    "t_stat_sale_price_Prev = mod1.tvalues['sale_price_Prev']\n",
    "print(\"T-stat for sale_price_Prev:\", t_stat_sale_price_Prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7358a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'property_id', 'sale_price', 'building_year_built',\n",
       "       'zip_code', 'Address_Google', 'closest_zip_1', 'distance_1',\n",
       "       'closest_zip_2', 'distance_2', 'closest_zip_3', 'distance_3', 'Month',\n",
       "       'Year', 'Returns', 'California AGI', 'City_Name', 'Unemployment Rate',\n",
       "       'Labor Force', 'Date', 'property_type_SINGLE FAMILY DWELLING',\n",
       "       'property_type_SINGLE FAMILY RESIDENCE', 'Zip_month',\n",
       "       'zip_1_Unemployment Rate_1', 'zip_1_Unemployment Rate_2',\n",
       "       'zip_1_Unemployment Rate_3', 'zip_1_sale_price_1', 'zip_1_sale_price_2',\n",
       "       'zip_1_sale_price_3', 'zip_1_California AGI_1',\n",
       "       'zip_1_California AGI_2', 'zip_1_California AGI_3',\n",
       "       'zip_1_Labor Force_1', 'zip_1_Labor Force_2', 'zip_1_Labor Force_3',\n",
       "       'zip_2_Unemployment Rate_1', 'zip_2_Unemployment Rate_2',\n",
       "       'zip_2_Unemployment Rate_3', 'zip_2_sale_price_1', 'zip_2_sale_price_2',\n",
       "       'zip_2_sale_price_3', 'zip_2_California AGI_1',\n",
       "       'zip_2_California AGI_2', 'zip_2_California AGI_3',\n",
       "       'zip_2_Labor Force_1', 'zip_2_Labor Force_2', 'zip_2_Labor Force_3',\n",
       "       'zip_3_Unemployment Rate_1', 'zip_3_Unemployment Rate_2',\n",
       "       'zip_3_Unemployment Rate_3', 'zip_3_sale_price_1', 'zip_3_sale_price_2',\n",
       "       'zip_3_sale_price_3', 'zip_3_California AGI_1',\n",
       "       'zip_3_California AGI_2', 'zip_3_California AGI_3',\n",
       "       'zip_3_Labor Force_1', 'zip_3_Labor Force_2', 'zip_3_Labor Force_3',\n",
       "       'Unemployment Rate_1', 'Unemployment Rate_2', 'Unemployment Rate_3',\n",
       "       'sale_price_1', 'sale_price_2', 'sale_price_3', 'California AGI_1',\n",
       "       'California AGI_2', 'California AGI_3', 'Labor Force_1',\n",
       "       'Labor Force_2', 'Labor Force_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec73ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>property_id</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>building_year_built</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>Address_Google</th>\n",
       "      <th>closest_zip_1</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>closest_zip_2</th>\n",
       "      <th>distance_2</th>\n",
       "      <th>...</th>\n",
       "      <th>sale_price_1</th>\n",
       "      <th>sale_price_2</th>\n",
       "      <th>sale_price_3</th>\n",
       "      <th>California AGI_1</th>\n",
       "      <th>California AGI_2</th>\n",
       "      <th>California AGI_3</th>\n",
       "      <th>Labor Force_1</th>\n",
       "      <th>Labor Force_2</th>\n",
       "      <th>Labor Force_3</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2506034039</td>\n",
       "      <td>240002</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12626 SAN FERNANDO RD, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2505017041</td>\n",
       "      <td>331000</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12938 DE HAVEN AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2530002019</td>\n",
       "      <td>299000</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>11521 HELA AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2506029006</td>\n",
       "      <td>200002</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12765 WOODCOCK AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2506035025</td>\n",
       "      <td>275002</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>91342</td>\n",
       "      <td>12577 RALSTON AVE, LOS ANGELES, CA 91342, USA</td>\n",
       "      <td>91340</td>\n",
       "      <td>1.58</td>\n",
       "      <td>91344</td>\n",
       "      <td>2.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  property_id  sale_price  building_year_built  zip_code  \\\n",
       "0           0   2506034039      240002               1943.0     91342   \n",
       "1           1   2505017041      331000               1965.0     91342   \n",
       "2           2   2530002019      299000               1958.0     91342   \n",
       "3           3   2506029006      200002               1948.0     91342   \n",
       "4           4   2506035025      275002               1947.0     91342   \n",
       "\n",
       "                                      Address_Google  closest_zip_1  \\\n",
       "0  12626 SAN FERNANDO RD, LOS ANGELES, CA 91342, USA          91340   \n",
       "1     12938 DE HAVEN AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "2         11521 HELA AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "3     12765 WOODCOCK AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "4      12577 RALSTON AVE, LOS ANGELES, CA 91342, USA          91340   \n",
       "\n",
       "   distance_1  closest_zip_2  distance_2  ...  sale_price_1  sale_price_2  \\\n",
       "0        1.58          91344        2.94  ...           0.0           0.0   \n",
       "1        1.58          91344        2.94  ...           0.0           0.0   \n",
       "2        1.58          91344        2.94  ...           0.0           0.0   \n",
       "3        1.58          91344        2.94  ...           0.0           0.0   \n",
       "4        1.58          91344        2.94  ...           0.0           0.0   \n",
       "\n",
       "   sale_price_3  California AGI_1  California AGI_2  California AGI_3  \\\n",
       "0           0.0               0.0               0.0               0.0   \n",
       "1           0.0               0.0               0.0               0.0   \n",
       "2           0.0               0.0               0.0               0.0   \n",
       "3           0.0               0.0               0.0               0.0   \n",
       "4           0.0               0.0               0.0               0.0   \n",
       "\n",
       "  Labor Force_1  Labor Force_2  Labor Force_3 Quarter  \n",
       "0           0.0            0.0            0.0       2  \n",
       "1           0.0            0.0            0.0       2  \n",
       "2           0.0            0.0            0.0       2  \n",
       "3           0.0            0.0            0.0       2  \n",
       "4           0.0            0.0            0.0       2  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Quarter'] = data_df['Month'].apply(lambda x: 4 if x >= 9 else (3 if x >= 6 else (2 if x >= 2 else 1)))\n",
    "data_df.dropna(inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba51877",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sort_values(by='Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34692213",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(columns=['Unnamed: 0', 'property_id', 'Address_Google', 'Month',\n",
    "       'Year','Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f1d7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(columns=['Zip_month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e8f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04d70d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.get_dummies(data_df, columns=['zip_code'], drop_first=True, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4979c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop(columns=['closest_zip_1', 'closest_zip_2', 'closest_zip_3' ,'City_Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41e7fac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562483"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.6 * len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "089085da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 20:22:39.403121: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-23 20:22:39.404493: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-23 20:23:06.838074: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 20:23:09.359882: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26367/26367 [==============================] - ETA: 0s - loss: 731882848256.0000 - mae: 619631.5625 - mse: 1045546139648.0000\n",
      "Epoch 1: loss improved from inf to 731882848256.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 376s 14ms/step - loss: 731882848256.0000 - mae: 619631.5625 - mse: 1045546139648.0000 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 668343205888.0000 - mae: 559784.1875 - mse: 954777010176.0000\n",
      "Epoch 2: loss improved from 731882848256.00000 to 668343205888.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 381s 14ms/step - loss: 668343205888.0000 - mae: 559784.1875 - mse: 954777010176.0000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 567456038912.0000 - mae: 456925.5000 - mse: 810651484160.0000\n",
      "Epoch 3: loss improved from 668343205888.00000 to 567479435264.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 381s 14ms/step - loss: 567479435264.0000 - mae: 456926.6875 - mse: 810684841984.0000 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 464563470336.0000 - mae: 354443.1250 - mse: 663663214592.0000\n",
      "Epoch 4: loss improved from 567479435264.00000 to 464563470336.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 381s 14ms/step - loss: 464563470336.0000 - mae: 354443.1250 - mse: 663663214592.0000 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 396596379648.0000 - mae: 315853.9688 - mse: 566569598976.0000\n",
      "Epoch 5: loss improved from 464563470336.00000 to 396601196544.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 349s 13ms/step - loss: 396601196544.0000 - mae: 315857.7812 - mse: 566576545792.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 372210892800.0000 - mae: 326745.9688 - mse: 531731021824.0000\n",
      "Epoch 6: loss improved from 396601196544.00000 to 372210892800.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 286s 11ms/step - loss: 372210892800.0000 - mae: 326745.9688 - mse: 531731021824.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 366625259520.0000 - mae: 334118.3750 - mse: 523750604800.0000\n",
      "Epoch 7: loss improved from 372210892800.00000 to 366612643840.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 300s 11ms/step - loss: 366612643840.0000 - mae: 334115.6875 - mse: 523732582400.0000 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 363096440832.0000 - mae: 335094.0938 - mse: 518708494336.0000\n",
      "Epoch 8: loss improved from 366612643840.00000 to 363089395712.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 297s 11ms/step - loss: 363089395712.0000 - mae: 335089.4688 - mse: 518698401792.0000 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 360871821312.0000 - mae: 335127.4062 - mse: 515531538432.0000\n",
      "Epoch 9: loss improved from 363089395712.00000 to 360865169408.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 309s 12ms/step - loss: 360865169408.0000 - mae: 335126.1875 - mse: 515522035712.0000 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 358318112768.0000 - mae: 334263.7812 - mse: 511881478144.0000\n",
      "Epoch 10: loss improved from 360865169408.00000 to 358318112768.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 326s 12ms/step - loss: 358318112768.0000 - mae: 334263.7812 - mse: 511881478144.0000 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 356730634240.0000 - mae: 333317.7188 - mse: 509615800320.0000\n",
      "Epoch 11: loss improved from 358318112768.00000 to 356716642304.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 308s 12ms/step - loss: 356716642304.0000 - mae: 333315.2812 - mse: 509595844608.0000 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 355921985536.0000 - mae: 332802.9375 - mse: 508460138496.0000\n",
      "Epoch 12: loss improved from 356716642304.00000 to 355921985536.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 311s 12ms/step - loss: 355921985536.0000 - mae: 332802.9375 - mse: 508460138496.0000 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 354750496768.0000 - mae: 332011.1875 - mse: 506787004416.0000\n",
      "Epoch 13: loss improved from 355921985536.00000 to 354735456256.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 336s 13ms/step - loss: 354735456256.0000 - mae: 332006.7812 - mse: 506765475840.0000 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 353429848064.0000 - mae: 332086.3750 - mse: 504901730304.0000\n",
      "Epoch 14: loss improved from 354735456256.00000 to 353429848064.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 310s 12ms/step - loss: 353429848064.0000 - mae: 332086.3750 - mse: 504901730304.0000 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 353056849920.0000 - mae: 331625.5000 - mse: 504366989312.0000\n",
      "Epoch 15: loss improved from 353429848064.00000 to 353048952832.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 309s 12ms/step - loss: 353048952832.0000 - mae: 331625.4688 - mse: 504355684352.0000 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 352493240320.0000 - mae: 331621.3125 - mse: 503560306688.0000\n",
      "Epoch 16: loss improved from 353048952832.00000 to 352493240320.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 277s 11ms/step - loss: 352493240320.0000 - mae: 331621.3125 - mse: 503560306688.0000 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 351508201472.0000 - mae: 330767.3750 - mse: 502153445376.0000\n",
      "Epoch 17: loss improved from 352493240320.00000 to 351562039296.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 351562039296.0000 - mae: 330781.2812 - mse: 502230286336.0000 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 350666457088.0000 - mae: 330118.5312 - mse: 500951187456.0000\n",
      "Epoch 18: loss improved from 351562039296.00000 to 350647320576.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 285s 11ms/step - loss: 350647320576.0000 - mae: 330115.5000 - mse: 500923924480.0000 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 350240931840.0000 - mae: 329916.6875 - mse: 500343570432.0000\n",
      "Epoch 19: loss improved from 350647320576.00000 to 350229626880.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 298s 11ms/step - loss: 350229626880.0000 - mae: 329915.0000 - mse: 500327448576.0000 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 345861259264.0000 - mae: 325662.2188 - mse: 494087634944.0000\n",
      "Epoch 20: loss improved from 350229626880.00000 to 345861259264.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 314s 12ms/step - loss: 345861259264.0000 - mae: 325662.2188 - mse: 494087634944.0000 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 345091538944.0000 - mae: 325598.0312 - mse: 492987711488.0000\n",
      "Epoch 21: loss improved from 345861259264.00000 to 345090326528.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 298s 11ms/step - loss: 345090326528.0000 - mae: 325599.2812 - mse: 492986007552.0000 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 344418615296.0000 - mae: 324411.8750 - mse: 492029214720.0000\n",
      "Epoch 22: loss improved from 345090326528.00000 to 344387092480.00000, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26367/26367 [==============================] - 371s 14ms/step - loss: 344387092480.0000 - mae: 324405.8125 - mse: 491984224256.0000 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 343815880704.0000 - mae: 324297.6250 - mse: 491166171136.0000\n",
      "Epoch 23: loss improved from 344387092480.00000 to 343863754752.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 2173s 82ms/step - loss: 343863754752.0000 - mae: 324306.0625 - mse: 491234656256.0000 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 343669145600.0000 - mae: 323517.2812 - mse: 490956193792.0000\n",
      "Epoch 24: loss improved from 343863754752.00000 to 343669145600.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 3562s 135ms/step - loss: 343669145600.0000 - mae: 323517.2812 - mse: 490956193792.0000 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 343844126720.0000 - mae: 323973.6250 - mse: 491206574080.0000\n",
      "Epoch 25: loss did not improve from 343669145600.00000\n",
      "26367/26367 [==============================] - 298s 11ms/step - loss: 343839604736.0000 - mae: 323975.6250 - mse: 491200118784.0000 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 343469260800.0000 - mae: 323815.3438 - mse: 490670030848.0000\n",
      "Epoch 26: loss improved from 343669145600.00000 to 343463395328.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1245s 47ms/step - loss: 343463395328.0000 - mae: 323816.3125 - mse: 490661609472.0000 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 343483711488.0000 - mae: 323195.3438 - mse: 490692411392.0000\n",
      "Epoch 27: loss did not improve from 343463395328.00000\n",
      "26367/26367 [==============================] - 2088s 79ms/step - loss: 343485513728.0000 - mae: 323194.4062 - mse: 490694934528.0000 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 342835757056.0000 - mae: 323235.9688 - mse: 489763962880.0000\n",
      "Epoch 28: loss improved from 343463395328.00000 to 342835757056.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 2517s 95ms/step - loss: 342835757056.0000 - mae: 323235.9688 - mse: 489763962880.0000 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 342465249280.0000 - mae: 323020.7812 - mse: 489237512192.0000\n",
      "Epoch 29: loss improved from 342835757056.00000 to 342465249280.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1204s 46ms/step - loss: 342465249280.0000 - mae: 323020.7812 - mse: 489237512192.0000 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 342638166016.0000 - mae: 322757.6562 - mse: 489481666560.0000\n",
      "Epoch 30: loss did not improve from 342465249280.00000\n",
      "26367/26367 [==============================] - 1364s 52ms/step - loss: 342633644032.0000 - mae: 322756.2500 - mse: 489475244032.0000 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 341926281216.0000 - mae: 322529.3125 - mse: 488463695872.0000\n",
      "Epoch 31: loss improved from 342465249280.00000 to 341966225408.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1269s 48ms/step - loss: 341966225408.0000 - mae: 322536.5312 - mse: 488520744960.0000 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 342370320384.0000 - mae: 322966.2188 - mse: 489100869632.0000\n",
      "Epoch 32: loss did not improve from 341966225408.00000\n",
      "26367/26367 [==============================] - 288s 11ms/step - loss: 342368911360.0000 - mae: 322966.9062 - mse: 489098903552.0000 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 342142091264.0000 - mae: 322175.5938 - mse: 488773517312.0000\n",
      "Epoch 33: loss did not improve from 341966225408.00000\n",
      "26367/26367 [==============================] - 1716s 65ms/step - loss: 342142091264.0000 - mae: 322175.5938 - mse: 488773517312.0000 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 341806415872.0000 - mae: 321980.5938 - mse: 488293662720.0000\n",
      "Epoch 34: loss improved from 341966225408.00000 to 341806415872.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 2302s 87ms/step - loss: 341806415872.0000 - mae: 321980.5938 - mse: 488293662720.0000 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 341656010752.0000 - mae: 321817.5625 - mse: 488078671872.0000\n",
      "Epoch 35: loss improved from 341806415872.00000 to 341656010752.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 328s 12ms/step - loss: 341656010752.0000 - mae: 321817.5625 - mse: 488078671872.0000 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 341228421120.0000 - mae: 322199.5938 - mse: 487472300032.0000\n",
      "Epoch 36: loss improved from 341656010752.00000 to 341228421120.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1492s 57ms/step - loss: 341228421120.0000 - mae: 322199.5938 - mse: 487472300032.0000 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 341482962944.0000 - mae: 321275.4062 - mse: 487833305088.0000\n",
      "Epoch 37: loss did not improve from 341228421120.00000\n",
      "26367/26367 [==============================] - 1298s 49ms/step - loss: 341482962944.0000 - mae: 321275.4062 - mse: 487833305088.0000 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 341634154496.0000 - mae: 321671.7188 - mse: 488051310592.0000\n",
      "Epoch 38: loss did not improve from 341228421120.00000\n",
      "26367/26367 [==============================] - 3633s 138ms/step - loss: 341634154496.0000 - mae: 321671.7188 - mse: 488051310592.0000 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 341131460608.0000 - mae: 321445.3750 - mse: 487330086912.0000\n",
      "Epoch 39: loss improved from 341228421120.00000 to 341135818752.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 351s 13ms/step - loss: 341135818752.0000 - mae: 321446.3750 - mse: 487336280064.0000 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 339277447168.0000 - mae: 319715.5312 - mse: 484681449472.0000\n",
      "Epoch 40: loss improved from 341135818752.00000 to 339277447168.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1232s 47ms/step - loss: 339277447168.0000 - mae: 319715.5312 - mse: 484681449472.0000 - lr: 2.5000e-04\n",
      "Epoch 41/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338744508416.0000 - mae: 318901.8125 - mse: 483921526784.0000\n",
      "Epoch 41: loss improved from 339277447168.00000 to 338744508416.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1288s 49ms/step - loss: 338744508416.0000 - mae: 318901.8125 - mse: 483921526784.0000 - lr: 2.5000e-04\n",
      "Epoch 42/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 339050725376.0000 - mae: 319510.1250 - mse: 484358946816.0000\n",
      "Epoch 42: loss did not improve from 338744508416.00000\n",
      "26367/26367 [==============================] - 1334s 51ms/step - loss: 339051872256.0000 - mae: 319507.7812 - mse: 484360552448.0000 - lr: 2.5000e-04\n",
      "Epoch 43/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338610552832.0000 - mae: 318893.7500 - mse: 483731734528.0000\n",
      "Epoch 43: loss improved from 338744508416.00000 to 338610552832.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1928s 73ms/step - loss: 338610552832.0000 - mae: 318893.7500 - mse: 483731734528.0000 - lr: 2.5000e-04\n",
      "Epoch 44/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 338547408896.0000 - mae: 318954.4688 - mse: 483639656448.0000\n",
      "Epoch 44: loss improved from 338610552832.00000 to 338539479040.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 496s 19ms/step - loss: 338539479040.0000 - mae: 318955.0312 - mse: 483628318720.0000 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338533482496.0000 - mae: 318868.6562 - mse: 483618816000.0000\n",
      "Epoch 45: loss improved from 338539479040.00000 to 338533482496.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 1230s 47ms/step - loss: 338533482496.0000 - mae: 318868.6562 - mse: 483618816000.0000 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338352340992.0000 - mae: 319092.7188 - mse: 483363061760.0000\n",
      "Epoch 46: loss improved from 338533482496.00000 to 338352340992.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 544s 21ms/step - loss: 338352340992.0000 - mae: 319092.7188 - mse: 483363061760.0000 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338435506176.0000 - mae: 318585.9688 - mse: 483480895488.0000\n",
      "Epoch 47: loss did not improve from 338352340992.00000\n",
      "26367/26367 [==============================] - 326s 12ms/step - loss: 338435506176.0000 - mae: 318585.9688 - mse: 483480895488.0000 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 338473713664.0000 - mae: 318998.5625 - mse: 483537977344.0000\n",
      "Epoch 48: loss did not improve from 338352340992.00000\n",
      "26367/26367 [==============================] - 314s 12ms/step - loss: 338500321280.0000 - mae: 319006.8438 - mse: 483575988224.0000 - lr: 2.5000e-04\n",
      "Epoch 49/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338323734528.0000 - mae: 318568.6875 - mse: 483322396672.0000\n",
      "Epoch 49: loss improved from 338352340992.00000 to 338323734528.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 334s 13ms/step - loss: 338323734528.0000 - mae: 318568.6875 - mse: 483322396672.0000 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338315116544.0000 - mae: 318748.7812 - mse: 483307421696.0000\n",
      "Epoch 50: loss improved from 338323734528.00000 to 338315116544.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 343s 13ms/step - loss: 338315116544.0000 - mae: 318748.7812 - mse: 483307421696.0000 - lr: 2.5000e-04\n",
      "Epoch 51/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 337956798464.0000 - mae: 318438.9062 - mse: 482793521152.0000\n",
      "Epoch 51: loss improved from 338315116544.00000 to 337956798464.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 343s 13ms/step - loss: 337956798464.0000 - mae: 318438.9062 - mse: 482793521152.0000 - lr: 2.5000e-04\n",
      "Epoch 52/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 338390745088.0000 - mae: 318565.6562 - mse: 483415556096.0000\n",
      "Epoch 52: loss did not improve from 337956798464.00000\n",
      "26367/26367 [==============================] - 326s 12ms/step - loss: 338390745088.0000 - mae: 318565.6562 - mse: 483415556096.0000 - lr: 2.5000e-04\n",
      "Epoch 53/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 337909579776.0000 - mae: 318269.5625 - mse: 482726641664.0000\n",
      "Epoch 53: loss improved from 337956798464.00000 to 337909579776.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 318s 12ms/step - loss: 337909579776.0000 - mae: 318269.8438 - mse: 482726608896.0000 - lr: 2.5000e-04\n",
      "Epoch 54/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 337726636032.0000 - mae: 318516.3438 - mse: 482468134912.0000\n",
      "Epoch 54: loss improved from 337909579776.00000 to 337726636032.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 347s 13ms/step - loss: 337726636032.0000 - mae: 318516.3438 - mse: 482468134912.0000 - lr: 2.5000e-04\n",
      "Epoch 55/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 337613750272.0000 - mae: 318170.5625 - mse: 482304720896.0000\n",
      "Epoch 55: loss improved from 337726636032.00000 to 337613750272.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 305s 12ms/step - loss: 337613750272.0000 - mae: 318170.5625 - mse: 482304720896.0000 - lr: 2.5000e-04\n",
      "Epoch 56/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 337754685440.0000 - mae: 317744.7188 - mse: 482506178560.0000\n",
      "Epoch 56: loss did not improve from 337613750272.00000\n",
      "26367/26367 [==============================] - 299s 11ms/step - loss: 337752653824.0000 - mae: 317743.3438 - mse: 482503294976.0000 - lr: 2.5000e-04\n",
      "Epoch 57/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 337477828608.0000 - mae: 318158.8125 - mse: 482111553536.0000\n",
      "Epoch 57: loss improved from 337613750272.00000 to 337530028032.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 315s 12ms/step - loss: 337530028032.0000 - mae: 318170.2188 - mse: 482186133504.0000 - lr: 2.5000e-04\n",
      "Epoch 58/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 337372086272.0000 - mae: 317898.7812 - mse: 481957183488.0000\n",
      "Epoch 58: loss improved from 337530028032.00000 to 337357406208.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 264s 10ms/step - loss: 337357406208.0000 - mae: 317892.9688 - mse: 481936211968.0000 - lr: 2.5000e-04\n",
      "Epoch 59/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 337559257088.0000 - mae: 317919.3438 - mse: 482225750016.0000\n",
      "Epoch 59: loss did not improve from 337357406208.00000\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 337559257088.0000 - mae: 317919.3438 - mse: 482225750016.0000 - lr: 2.5000e-04\n",
      "Epoch 60/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 336577167360.0000 - mae: 317144.8750 - mse: 480824852480.0000\n",
      "Epoch 60: loss improved from 337357406208.00000 to 336577167360.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 336577167360.0000 - mae: 317144.8750 - mse: 480824852480.0000 - lr: 1.2500e-04\n",
      "Epoch 61/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 336701161472.0000 - mae: 317147.7500 - mse: 481002979328.0000\n",
      "Epoch 61: loss did not improve from 336577167360.00000\n",
      "26367/26367 [==============================] - 261s 10ms/step - loss: 336685694976.0000 - mae: 317143.7812 - mse: 480980959232.0000 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 336203710464.0000 - mae: 316899.1562 - mse: 480292241408.0000\n",
      "Epoch 62: loss improved from 336577167360.00000 to 336173236224.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 336173236224.0000 - mae: 316891.6875 - mse: 480248659968.0000 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 336215212032.0000 - mae: 316421.4688 - mse: 480304857088.0000\n",
      "Epoch 63: loss did not improve from 336173236224.00000\n",
      "26367/26367 [==============================] - 270s 10ms/step - loss: 336220422144.0000 - mae: 316420.6250 - mse: 480312295424.0000 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 336331833344.0000 - mae: 316617.0312 - mse: 480470630400.0000\n",
      "Epoch 64: loss did not improve from 336173236224.00000\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 336331833344.0000 - mae: 316617.0312 - mse: 480470630400.0000 - lr: 1.2500e-04\n",
      "Epoch 65/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 336100753408.0000 - mae: 316772.0625 - mse: 480146489344.0000\n",
      "Epoch 65: loss improved from 336173236224.00000 to 336097574912.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 336097574912.0000 - mae: 316773.0938 - mse: 480141934592.0000 - lr: 1.2500e-04\n",
      "Epoch 66/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 335977283584.0000 - mae: 316494.3438 - mse: 479968624640.0000\n",
      "Epoch 66: loss improved from 336097574912.00000 to 335971713024.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 255s 10ms/step - loss: 335971713024.0000 - mae: 316495.2812 - mse: 479960662016.0000 - lr: 1.2500e-04\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26365/26367 [============================>.] - ETA: 0s - loss: 336024698880.0000 - mae: 316654.1562 - mse: 480036257792.0000\n",
      "Epoch 67: loss did not improve from 335971713024.00000\n",
      "26367/26367 [==============================] - 257s 10ms/step - loss: 336018964480.0000 - mae: 316651.6562 - mse: 480028098560.0000 - lr: 1.2500e-04\n",
      "Epoch 68/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 336279764992.0000 - mae: 316677.2188 - mse: 480400670720.0000\n",
      "Epoch 68: loss did not improve from 335971713024.00000\n",
      "26367/26367 [==============================] - 255s 10ms/step - loss: 336266100736.0000 - mae: 316671.7812 - mse: 480381206528.0000 - lr: 1.2500e-04\n",
      "Epoch 69/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 335955689472.0000 - mae: 316325.3750 - mse: 479936905216.0000\n",
      "Epoch 69: loss improved from 335971713024.00000 to 335955689472.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 266s 10ms/step - loss: 335955689472.0000 - mae: 316325.3750 - mse: 479936905216.0000 - lr: 1.2500e-04\n",
      "Epoch 70/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 335865708544.0000 - mae: 316600.6562 - mse: 479808749568.0000\n",
      "Epoch 70: loss improved from 335955689472.00000 to 335837691904.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 335837691904.0000 - mae: 316596.0625 - mse: 479768674304.0000 - lr: 1.2500e-04\n",
      "Epoch 71/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 335871148032.0000 - mae: 316170.4062 - mse: 479814549504.0000\n",
      "Epoch 71: loss did not improve from 335837691904.00000\n",
      "26367/26367 [==============================] - 262s 10ms/step - loss: 335866068992.0000 - mae: 316171.8438 - mse: 479807340544.0000 - lr: 1.2500e-04\n",
      "Epoch 72/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 335749709824.0000 - mae: 316187.5625 - mse: 479641960448.0000\n",
      "Epoch 72: loss improved from 335837691904.00000 to 335738503168.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 258s 10ms/step - loss: 335738503168.0000 - mae: 316184.9688 - mse: 479625936896.0000 - lr: 1.2500e-04\n",
      "Epoch 73/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 335642755072.0000 - mae: 316282.7812 - mse: 479488737280.0000\n",
      "Epoch 73: loss improved from 335738503168.00000 to 335640363008.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 335640363008.0000 - mae: 316284.3125 - mse: 479485329408.0000 - lr: 1.2500e-04\n",
      "Epoch 74/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 335794438144.0000 - mae: 315807.5000 - mse: 479707463680.0000\n",
      "Epoch 74: loss did not improve from 335640363008.00000\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 335794438144.0000 - mae: 315807.5000 - mse: 479707463680.0000 - lr: 1.2500e-04\n",
      "Epoch 75/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 335675883520.0000 - mae: 316218.3438 - mse: 479538642944.0000\n",
      "Epoch 75: loss did not improve from 335640363008.00000\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 335681748992.0000 - mae: 316220.8125 - mse: 479547031552.0000 - lr: 1.2500e-04\n",
      "Epoch 76/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 335978823680.0000 - mae: 316305.3438 - mse: 479968788480.0000\n",
      "Epoch 76: loss did not improve from 335640363008.00000\n",
      "26367/26367 [==============================] - 263s 10ms/step - loss: 335957884928.0000 - mae: 316302.6562 - mse: 479938904064.0000 - lr: 1.2500e-04\n",
      "Epoch 77/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 335558180864.0000 - mae: 316104.2188 - mse: 479368183808.0000\n",
      "Epoch 77: loss improved from 335640363008.00000 to 335579283456.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 257s 10ms/step - loss: 335579283456.0000 - mae: 316114.3750 - mse: 479398297600.0000 - lr: 1.2500e-04\n",
      "Epoch 78/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 335368912896.0000 - mae: 316139.0000 - mse: 479096438784.0000\n",
      "Epoch 78: loss improved from 335579283456.00000 to 335370846208.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 258s 10ms/step - loss: 335370846208.0000 - mae: 316143.5938 - mse: 479099224064.0000 - lr: 1.2500e-04\n",
      "Epoch 79/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 335559196672.0000 - mae: 315804.7812 - mse: 479371362304.0000\n",
      "Epoch 79: loss did not improve from 335370846208.00000\n",
      "26367/26367 [==============================] - 259s 10ms/step - loss: 335555100672.0000 - mae: 315803.9062 - mse: 479365464064.0000 - lr: 1.2500e-04\n",
      "Epoch 80/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 334574649344.0000 - mae: 315205.6875 - mse: 477961486336.0000\n",
      "Epoch 80: loss improved from 335370846208.00000 to 334574649344.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 258s 10ms/step - loss: 334574649344.0000 - mae: 315205.6875 - mse: 477961486336.0000 - lr: 6.2500e-05\n",
      "Epoch 81/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 334257553408.0000 - mae: 314923.9375 - mse: 477508370432.0000\n",
      "Epoch 81: loss improved from 334574649344.00000 to 334244708352.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 266s 10ms/step - loss: 334244708352.0000 - mae: 314920.4375 - mse: 477490020352.0000 - lr: 6.2500e-05\n",
      "Epoch 82/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 334427521024.0000 - mae: 315275.7500 - mse: 477751541760.0000\n",
      "Epoch 82: loss did not improve from 334244708352.00000\n",
      "26367/26367 [==============================] - 294s 11ms/step - loss: 334427521024.0000 - mae: 315275.7500 - mse: 477751541760.0000 - lr: 6.2500e-05\n",
      "Epoch 83/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 334415790080.0000 - mae: 315009.8438 - mse: 477736665088.0000\n",
      "Epoch 83: loss did not improve from 334244708352.00000\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 334413561856.0000 - mae: 315010.8750 - mse: 477733519360.0000 - lr: 6.2500e-05\n",
      "Epoch 84/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 334205681664.0000 - mae: 314799.2188 - mse: 477438738432.0000\n",
      "Epoch 84: loss improved from 334244708352.00000 to 334205681664.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 276s 10ms/step - loss: 334205681664.0000 - mae: 314799.2188 - mse: 477438738432.0000 - lr: 6.2500e-05\n",
      "Epoch 85/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 334456422400.0000 - mae: 314822.3750 - mse: 477793845248.0000\n",
      "Epoch 85: loss did not improve from 334205681664.00000\n",
      "26367/26367 [==============================] - 283s 11ms/step - loss: 334452260864.0000 - mae: 314821.0625 - mse: 477787947008.0000 - lr: 6.2500e-05\n",
      "Epoch 86/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 334369488896.0000 - mae: 315473.1875 - mse: 477671718912.0000\n",
      "Epoch 86: loss did not improve from 334205681664.00000\n",
      "26367/26367 [==============================] - 271s 10ms/step - loss: 334380007424.0000 - mae: 315477.4375 - mse: 477686759424.0000 - lr: 6.2500e-05\n",
      "Epoch 87/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 334061305856.0000 - mae: 315063.8750 - mse: 477231415296.0000\n",
      "Epoch 87: loss improved from 334205681664.00000 to 334080966656.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 334080966656.0000 - mae: 315068.3438 - mse: 477259497472.0000 - lr: 6.2500e-05\n",
      "Epoch 88/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 333647478784.0000 - mae: 315137.5625 - mse: 476636708864.0000\n",
      "Epoch 88: loss improved from 334080966656.00000 to 333647478784.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 278s 11ms/step - loss: 333647478784.0000 - mae: 315137.5625 - mse: 476636708864.0000 - lr: 6.2500e-05\n",
      "Epoch 89/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 334104592384.0000 - mae: 315023.0312 - mse: 477292232704.0000\n",
      "Epoch 89: loss did not improve from 333647478784.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26367/26367 [==============================] - 269s 10ms/step - loss: 334092894208.0000 - mae: 315020.1875 - mse: 477275521024.0000 - lr: 6.2500e-05\n",
      "Epoch 90/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 334118486016.0000 - mae: 315168.3750 - mse: 477314580480.0000\n",
      "Epoch 90: loss did not improve from 333647478784.00000\n",
      "26367/26367 [==============================] - 279s 11ms/step - loss: 334192246784.0000 - mae: 315184.2500 - mse: 477420027904.0000 - lr: 6.2500e-05\n",
      "Epoch 91/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 334113603584.0000 - mae: 315092.4375 - mse: 477304291328.0000\n",
      "Epoch 91: loss did not improve from 333647478784.00000\n",
      "26367/26367 [==============================] - 292s 11ms/step - loss: 334113603584.0000 - mae: 315092.4375 - mse: 477304291328.0000 - lr: 6.2500e-05\n",
      "Epoch 92/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 333955825664.0000 - mae: 315161.5625 - mse: 477079142400.0000\n",
      "Epoch 92: loss did not improve from 333647478784.00000\n",
      "26367/26367 [==============================] - 273s 10ms/step - loss: 333941899264.0000 - mae: 315156.7812 - mse: 477059252224.0000 - lr: 6.2500e-05\n",
      "Epoch 93/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 334206730240.0000 - mae: 314897.1875 - mse: 477437526016.0000\n",
      "Epoch 93: loss did not improve from 333647478784.00000\n",
      "26367/26367 [==============================] - 272s 10ms/step - loss: 334203224064.0000 - mae: 314897.6250 - mse: 477432545280.0000 - lr: 6.2500e-05\n",
      "Epoch 94/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 333616414720.0000 - mae: 314681.9688 - mse: 476594536448.0000\n",
      "Epoch 94: loss improved from 333647478784.00000 to 333640597504.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 285s 11ms/step - loss: 333640597504.0000 - mae: 314686.6250 - mse: 476629073920.0000 - lr: 6.2500e-05\n",
      "Epoch 95/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 333716324352.0000 - mae: 314636.7188 - mse: 476738191360.0000\n",
      "Epoch 95: loss did not improve from 333640597504.00000\n",
      "26367/26367 [==============================] - 279s 11ms/step - loss: 333716324352.0000 - mae: 314636.7188 - mse: 476738191360.0000 - lr: 6.2500e-05\n",
      "Epoch 96/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 333885603840.0000 - mae: 314689.3438 - mse: 476980117504.0000\n",
      "Epoch 96: loss did not improve from 333640597504.00000\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 333885603840.0000 - mae: 314689.3438 - mse: 476980117504.0000 - lr: 6.2500e-05\n",
      "Epoch 97/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 333866008576.0000 - mae: 314615.7812 - mse: 476949643264.0000\n",
      "Epoch 97: loss did not improve from 333640597504.00000\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 333895467008.0000 - mae: 314620.7188 - mse: 476991717376.0000 - lr: 6.2500e-05\n",
      "Epoch 98/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 333625163776.0000 - mae: 314656.4062 - mse: 476605480960.0000\n",
      "Epoch 98: loss did not improve from 333640597504.00000\n",
      "26367/26367 [==============================] - 278s 11ms/step - loss: 333650722816.0000 - mae: 314664.4688 - mse: 476641951744.0000 - lr: 6.2500e-05\n",
      "Epoch 99/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 333627326464.0000 - mae: 314760.7812 - mse: 476608135168.0000\n",
      "Epoch 99: loss improved from 333640597504.00000 to 333625360384.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 333625360384.0000 - mae: 314760.2500 - mse: 476605284352.0000 - lr: 6.2500e-05\n",
      "Epoch 100/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 332791382016.0000 - mae: 314097.6250 - mse: 475415183360.0000\n",
      "Epoch 100: loss improved from 333625360384.00000 to 332783124480.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 332783124480.0000 - mae: 314101.5938 - mse: 475403386880.0000 - lr: 3.1250e-05\n",
      "Epoch 101/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 332524584960.0000 - mae: 313399.5000 - mse: 475034877952.0000\n",
      "Epoch 101: loss improved from 332783124480.00000 to 332524584960.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 264s 10ms/step - loss: 332524584960.0000 - mae: 313399.5000 - mse: 475034877952.0000 - lr: 3.1250e-05\n",
      "Epoch 102/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 332791250944.0000 - mae: 313535.7812 - mse: 475417706496.0000\n",
      "Epoch 102: loss did not improve from 332524584960.00000\n",
      "26367/26367 [==============================] - 251s 10ms/step - loss: 332791250944.0000 - mae: 313535.7812 - mse: 475417706496.0000 - lr: 3.1250e-05\n",
      "Epoch 103/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 332624560128.0000 - mae: 313386.2812 - mse: 475179515904.0000\n",
      "Epoch 103: loss did not improve from 332524584960.00000\n",
      "26367/26367 [==============================] - 237s 9ms/step - loss: 332624560128.0000 - mae: 313386.2812 - mse: 475179515904.0000 - lr: 3.1250e-05\n",
      "Epoch 104/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 332651921408.0000 - mae: 313641.6250 - mse: 475215953920.0000\n",
      "Epoch 104: loss did not improve from 332524584960.00000\n",
      "26367/26367 [==============================] - 236s 9ms/step - loss: 332652445696.0000 - mae: 313643.6562 - mse: 475216740352.0000 - lr: 3.1250e-05\n",
      "Epoch 105/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 332639764480.0000 - mae: 313875.2500 - mse: 475201208320.0000\n",
      "Epoch 105: loss did not improve from 332524584960.00000\n",
      "26367/26367 [==============================] - 236s 9ms/step - loss: 332628656128.0000 - mae: 313872.7188 - mse: 475185283072.0000 - lr: 3.1250e-05\n",
      "Epoch 106/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 332348194816.0000 - mae: 313387.6875 - mse: 474782203904.0000\n",
      "Epoch 106: loss improved from 332524584960.00000 to 332425396224.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 236s 9ms/step - loss: 332425396224.0000 - mae: 313405.0312 - mse: 474892533760.0000 - lr: 3.1250e-05\n",
      "Epoch 107/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 332505841664.0000 - mae: 313592.0312 - mse: 475007385600.0000\n",
      "Epoch 107: loss did not improve from 332425396224.00000\n",
      "26367/26367 [==============================] - 238s 9ms/step - loss: 332521504768.0000 - mae: 313603.9062 - mse: 475029733376.0000 - lr: 3.1250e-05\n",
      "Epoch 108/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 332891095040.0000 - mae: 313893.2812 - mse: 475557330944.0000\n",
      "Epoch 108: loss did not improve from 332425396224.00000\n",
      "26367/26367 [==============================] - 237s 9ms/step - loss: 332876251136.0000 - mae: 313888.4375 - mse: 475536162816.0000 - lr: 3.1250e-05\n",
      "Epoch 109/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 332566200320.0000 - mae: 313659.6250 - mse: 475092975616.0000\n",
      "Epoch 109: loss did not improve from 332425396224.00000\n",
      "26367/26367 [==============================] - 237s 9ms/step - loss: 332552044544.0000 - mae: 313653.8438 - mse: 475072790528.0000 - lr: 3.1250e-05\n",
      "Epoch 110/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 332419760128.0000 - mae: 313456.1250 - mse: 474883260416.0000\n",
      "Epoch 110: loss improved from 332425396224.00000 to 332396331008.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 232s 9ms/step - loss: 332396331008.0000 - mae: 313450.0000 - mse: 474849804288.0000 - lr: 3.1250e-05\n",
      "Epoch 111/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 332409077760.0000 - mae: 313676.1875 - mse: 474872283136.0000\n",
      "Epoch 111: loss did not improve from 332396331008.00000\n",
      "26367/26367 [==============================] - 232s 9ms/step - loss: 332397412352.0000 - mae: 313674.4062 - mse: 474855669760.0000 - lr: 3.1250e-05\n",
      "Epoch 112/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 332146475008.0000 - mae: 313669.9062 - mse: 474494828544.0000\n",
      "Epoch 112: loss improved from 332396331008.00000 to 332146475008.00000, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26367/26367 [==============================] - 239s 9ms/step - loss: 332146475008.0000 - mae: 313669.9062 - mse: 474494828544.0000 - lr: 3.1250e-05\n",
      "Epoch 113/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 332472057856.0000 - mae: 313850.0312 - mse: 474956693504.0000\n",
      "Epoch 113: loss did not improve from 332146475008.00000\n",
      "26367/26367 [==============================] - 233s 9ms/step - loss: 332468191232.0000 - mae: 313849.7188 - mse: 474951221248.0000 - lr: 3.1250e-05\n",
      "Epoch 114/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 332292456448.0000 - mae: 313498.8750 - mse: 474702184448.0000\n",
      "Epoch 114: loss did not improve from 332146475008.00000\n",
      "26367/26367 [==============================] - 233s 9ms/step - loss: 332267094016.0000 - mae: 313490.3750 - mse: 474665943040.0000 - lr: 3.1250e-05\n",
      "Epoch 115/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 332737118208.0000 - mae: 314147.0312 - mse: 475338506240.0000\n",
      "Epoch 115: loss did not improve from 332146475008.00000\n",
      "26367/26367 [==============================] - 233s 9ms/step - loss: 332738330624.0000 - mae: 314144.8125 - mse: 475340242944.0000 - lr: 3.1250e-05\n",
      "Epoch 116/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 332446892032.0000 - mae: 313907.1250 - mse: 474922975232.0000\n",
      "Epoch 116: loss did not improve from 332146475008.00000\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 332436144128.0000 - mae: 313902.8125 - mse: 474907639808.0000 - lr: 3.1250e-05\n",
      "Epoch 117/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 332073238528.0000 - mae: 313650.9375 - mse: 474389086208.0000\n",
      "Epoch 117: loss improved from 332146475008.00000 to 332063539200.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 227s 9ms/step - loss: 332063539200.0000 - mae: 313649.6250 - mse: 474375258112.0000 - lr: 3.1250e-05\n",
      "Epoch 118/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 332368740352.0000 - mae: 313681.3750 - mse: 474812841984.0000\n",
      "Epoch 118: loss did not improve from 332063539200.00000\n",
      "26367/26367 [==============================] - 229s 9ms/step - loss: 332363169792.0000 - mae: 313686.8438 - mse: 474804846592.0000 - lr: 3.1250e-05\n",
      "Epoch 119/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 332514295808.0000 - mae: 313793.2812 - mse: 475020066816.0000\n",
      "Epoch 119: loss did not improve from 332063539200.00000\n",
      "26367/26367 [==============================] - 229s 9ms/step - loss: 332514295808.0000 - mae: 313793.2812 - mse: 475020066816.0000 - lr: 3.1250e-05\n",
      "Epoch 120/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 331656036352.0000 - mae: 313252.8125 - mse: 473792348160.0000\n",
      "Epoch 120: loss improved from 332063539200.00000 to 331642339328.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 331642339328.0000 - mae: 313249.2812 - mse: 473772752896.0000 - lr: 1.5625e-05\n",
      "Epoch 121/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 331352244224.0000 - mae: 312962.0625 - mse: 473360171008.0000\n",
      "Epoch 121: loss improved from 331642339328.00000 to 331351883776.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 331351883776.0000 - mae: 312962.7500 - mse: 473359646720.0000 - lr: 1.5625e-05\n",
      "Epoch 122/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 331823054848.0000 - mae: 313280.6875 - mse: 474032537600.0000\n",
      "Epoch 122: loss did not improve from 331351883776.00000\n",
      "26367/26367 [==============================] - 229s 9ms/step - loss: 331823054848.0000 - mae: 313280.6875 - mse: 474032537600.0000 - lr: 1.5625e-05\n",
      "Epoch 123/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 331133911040.0000 - mae: 313202.3438 - mse: 473045532672.0000\n",
      "Epoch 123: loss improved from 331351883776.00000 to 331130175488.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 331130175488.0000 - mae: 313201.8125 - mse: 473040191488.0000 - lr: 1.5625e-05\n",
      "Epoch 124/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 331605999616.0000 - mae: 313233.2188 - mse: 473723895808.0000\n",
      "Epoch 124: loss did not improve from 331130175488.00000\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 331627888640.0000 - mae: 313235.9375 - mse: 473755189248.0000 - lr: 1.5625e-05\n",
      "Epoch 125/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 331067752448.0000 - mae: 312975.9375 - mse: 472951947264.0000\n",
      "Epoch 125: loss improved from 331130175488.00000 to 331053105152.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 331053105152.0000 - mae: 312970.9375 - mse: 472931041280.0000 - lr: 1.5625e-05\n",
      "Epoch 126/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 331340218368.0000 - mae: 313266.7812 - mse: 473342574592.0000\n",
      "Epoch 126: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 227s 9ms/step - loss: 331326947328.0000 - mae: 313262.4688 - mse: 473323601920.0000 - lr: 1.5625e-05\n",
      "Epoch 127/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 331116347392.0000 - mae: 312950.1250 - mse: 473025118208.0000\n",
      "Epoch 127: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 228s 9ms/step - loss: 331162615808.0000 - mae: 312964.1875 - mse: 473091244032.0000 - lr: 1.5625e-05\n",
      "Epoch 128/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 331204165632.0000 - mae: 312949.9688 - mse: 473146490880.0000\n",
      "Epoch 128: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 226s 9ms/step - loss: 331188207616.0000 - mae: 312945.8125 - mse: 473123684352.0000 - lr: 1.5625e-05\n",
      "Epoch 129/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 331267801088.0000 - mae: 313049.5625 - mse: 473240371200.0000\n",
      "Epoch 129: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 226s 9ms/step - loss: 331291787264.0000 - mae: 313054.7812 - mse: 473274646528.0000 - lr: 1.5625e-05\n",
      "Epoch 130/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 331184013312.0000 - mae: 313052.5938 - mse: 473122635776.0000\n",
      "Epoch 130: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 226s 9ms/step - loss: 331217076224.0000 - mae: 313057.0312 - mse: 473169788928.0000 - lr: 1.5625e-05\n",
      "Epoch 131/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 331265376256.0000 - mae: 313130.0312 - mse: 473239322624.0000\n",
      "Epoch 131: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 229s 9ms/step - loss: 331240996864.0000 - mae: 313127.6562 - mse: 473204490240.0000 - lr: 1.5625e-05\n",
      "Epoch 132/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 331333369856.0000 - mae: 313052.8750 - mse: 473332547584.0000\n",
      "Epoch 132: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 236s 9ms/step - loss: 331307974656.0000 - mae: 313047.8438 - mse: 473296371712.0000 - lr: 1.5625e-05\n",
      "Epoch 133/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 331149672448.0000 - mae: 313118.2812 - mse: 473070272512.0000\n",
      "Epoch 133: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 239s 9ms/step - loss: 331134795776.0000 - mae: 313114.4062 - mse: 473049006080.0000 - lr: 1.5625e-05\n",
      "Epoch 134/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 331525128192.0000 - mae: 313217.8125 - mse: 473603932160.0000\n",
      "Epoch 134: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 331501338624.0000 - mae: 313210.9688 - mse: 473569951744.0000 - lr: 1.5625e-05\n",
      "Epoch 135/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 331047763968.0000 - mae: 313152.0625 - mse: 472926388224.0000\n",
      "Epoch 135: loss did not improve from 331053105152.00000\n",
      "26367/26367 [==============================] - 271s 10ms/step - loss: 331102683136.0000 - mae: 313161.0312 - mse: 473004900352.0000 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330975281152.0000 - mae: 312782.7188 - mse: 472821596160.0000\n",
      "Epoch 136: loss improved from 331053105152.00000 to 330975281152.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 260s 10ms/step - loss: 330975281152.0000 - mae: 312782.7188 - mse: 472821596160.0000 - lr: 1.5625e-05\n",
      "Epoch 137/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 331177361408.0000 - mae: 312738.2812 - mse: 473108611072.0000\n",
      "Epoch 137: loss did not improve from 330975281152.00000\n",
      "26367/26367 [==============================] - 232s 9ms/step - loss: 331177361408.0000 - mae: 312738.2812 - mse: 473108611072.0000 - lr: 1.5625e-05\n",
      "Epoch 138/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 331258200064.0000 - mae: 312906.7812 - mse: 473222840320.0000\n",
      "Epoch 138: loss did not improve from 330975281152.00000\n",
      "26367/26367 [==============================] - 237s 9ms/step - loss: 331292377088.0000 - mae: 312917.3750 - mse: 473271697408.0000 - lr: 1.5625e-05\n",
      "Epoch 139/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 331071619072.0000 - mae: 312946.5000 - mse: 472958402560.0000\n",
      "Epoch 139: loss did not improve from 330975281152.00000\n",
      "26367/26367 [==============================] - 237s 9ms/step - loss: 331066966016.0000 - mae: 312944.3750 - mse: 472951750656.0000 - lr: 1.5625e-05\n",
      "Epoch 140/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 330912038912.0000 - mae: 312751.3750 - mse: 472732958720.0000\n",
      "Epoch 140: loss improved from 330975281152.00000 to 330900570112.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 251s 10ms/step - loss: 330900570112.0000 - mae: 312749.4062 - mse: 472716574720.0000 - lr: 7.8125e-06\n",
      "Epoch 141/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330592518144.0000 - mae: 312535.5312 - mse: 472273715200.0000\n",
      "Epoch 141: loss improved from 330900570112.00000 to 330588487680.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 267s 10ms/step - loss: 330588487680.0000 - mae: 312534.6875 - mse: 472267948032.0000 - lr: 7.8125e-06\n",
      "Epoch 142/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330694721536.0000 - mae: 312613.6875 - mse: 472423563264.0000\n",
      "Epoch 142: loss did not improve from 330588487680.00000\n",
      "26367/26367 [==============================] - 270s 10ms/step - loss: 330690592768.0000 - mae: 312613.0312 - mse: 472417697792.0000 - lr: 7.8125e-06\n",
      "Epoch 143/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330681221120.0000 - mae: 312629.6250 - mse: 472403574784.0000\n",
      "Epoch 143: loss did not improve from 330588487680.00000\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 330681221120.0000 - mae: 312629.6250 - mse: 472403574784.0000 - lr: 7.8125e-06\n",
      "Epoch 144/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330670014464.0000 - mae: 312583.2500 - mse: 472383356928.0000\n",
      "Epoch 144: loss did not improve from 330588487680.00000\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 330666737664.0000 - mae: 312583.0000 - mse: 472378638336.0000 - lr: 7.8125e-06\n",
      "Epoch 145/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330494902272.0000 - mae: 312710.2500 - mse: 472134221824.0000\n",
      "Epoch 145: loss improved from 330588487680.00000 to 330493394944.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 330493394944.0000 - mae: 312709.6250 - mse: 472132026368.0000 - lr: 7.8125e-06\n",
      "Epoch 146/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330758258688.0000 - mae: 312559.1875 - mse: 472509120512.0000\n",
      "Epoch 146: loss did not improve from 330493394944.00000\n",
      "26367/26367 [==============================] - 286s 11ms/step - loss: 330758258688.0000 - mae: 312559.1875 - mse: 472509120512.0000 - lr: 7.8125e-06\n",
      "Epoch 147/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330533339136.0000 - mae: 312425.4688 - mse: 472190353408.0000\n",
      "Epoch 147: loss did not improve from 330493394944.00000\n",
      "26367/26367 [==============================] - 296s 11ms/step - loss: 330579869696.0000 - mae: 312438.4375 - mse: 472256839680.0000 - lr: 7.8125e-06\n",
      "Epoch 148/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 330121183232.0000 - mae: 312246.0000 - mse: 471599906816.0000\n",
      "Epoch 148: loss improved from 330493394944.00000 to 330110205952.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 299s 11ms/step - loss: 330110205952.0000 - mae: 312245.9062 - mse: 471584210944.0000 - lr: 7.8125e-06\n",
      "Epoch 149/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330465738752.0000 - mae: 312438.8750 - mse: 472092344320.0000\n",
      "Epoch 149: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 283s 11ms/step - loss: 330465738752.0000 - mae: 312438.8750 - mse: 472092344320.0000 - lr: 7.8125e-06\n",
      "Epoch 150/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 330267197440.0000 - mae: 312666.3438 - mse: 471811227648.0000\n",
      "Epoch 150: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 268s 10ms/step - loss: 330350100480.0000 - mae: 312685.9375 - mse: 471929683968.0000 - lr: 7.8125e-06\n",
      "Epoch 151/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330275291136.0000 - mae: 312623.2188 - mse: 471822499840.0000\n",
      "Epoch 151: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 240s 9ms/step - loss: 330281549824.0000 - mae: 312626.5938 - mse: 471831478272.0000 - lr: 7.8125e-06\n",
      "Epoch 152/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 330553589760.0000 - mae: 312333.0000 - mse: 472217550848.0000\n",
      "Epoch 152: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 239s 9ms/step - loss: 330540711936.0000 - mae: 312329.1250 - mse: 472199135232.0000 - lr: 7.8125e-06\n",
      "Epoch 153/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 330293149696.0000 - mae: 312297.4375 - mse: 471847206912.0000\n",
      "Epoch 153: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 240s 9ms/step - loss: 330289741824.0000 - mae: 312296.4688 - mse: 471842324480.0000 - lr: 7.8125e-06\n",
      "Epoch 154/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 330417831936.0000 - mae: 312336.8438 - mse: 472024940544.0000\n",
      "Epoch 154: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 239s 9ms/step - loss: 330403610624.0000 - mae: 312335.8750 - mse: 472004624384.0000 - lr: 7.8125e-06\n",
      "Epoch 155/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 330495721472.0000 - mae: 312386.6562 - mse: 472134909952.0000\n",
      "Epoch 155: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 238s 9ms/step - loss: 330453221376.0000 - mae: 312373.2188 - mse: 472074223616.0000 - lr: 7.8125e-06\n",
      "Epoch 156/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330243801088.0000 - mae: 312528.8438 - mse: 471776886784.0000\n",
      "Epoch 156: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 237s 9ms/step - loss: 330243801088.0000 - mae: 312528.8438 - mse: 471776886784.0000 - lr: 7.8125e-06\n",
      "Epoch 157/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330568171520.0000 - mae: 312539.4062 - mse: 472240521216.0000\n",
      "Epoch 157: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 242s 9ms/step - loss: 330568171520.0000 - mae: 312539.4062 - mse: 472240521216.0000 - lr: 7.8125e-06\n",
      "Epoch 158/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 330578296832.0000 - mae: 312623.9062 - mse: 472255561728.0000\n",
      "Epoch 158: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 265s 10ms/step - loss: 330582032384.0000 - mae: 312625.9688 - mse: 472260902912.0000 - lr: 7.8125e-06\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26367/26367 [==============================] - ETA: 0s - loss: 330643767296.0000 - mae: 312637.0625 - mse: 472348065792.0000\n",
      "Epoch 159: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 275s 10ms/step - loss: 330643767296.0000 - mae: 312637.0625 - mse: 472348065792.0000 - lr: 7.8125e-06\n",
      "Epoch 160/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 330252681216.0000 - mae: 312462.5000 - mse: 471789535232.0000\n",
      "Epoch 160: loss did not improve from 330110205952.00000\n",
      "26367/26367 [==============================] - 283s 11ms/step - loss: 330245963776.0000 - mae: 312460.5312 - mse: 471779934208.0000 - lr: 3.9063e-06\n",
      "Epoch 161/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 329849143296.0000 - mae: 312112.3125 - mse: 471214063616.0000\n",
      "Epoch 161: loss improved from 330110205952.00000 to 329865199616.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 287s 11ms/step - loss: 329865199616.0000 - mae: 312116.0938 - mse: 471237001216.0000 - lr: 3.9063e-06\n",
      "Epoch 162/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329992404992.0000 - mae: 312242.5000 - mse: 471418306560.0000\n",
      "Epoch 162: loss did not improve from 329865199616.00000\n",
      "26367/26367 [==============================] - 286s 11ms/step - loss: 329992404992.0000 - mae: 312242.5000 - mse: 471418306560.0000 - lr: 3.9063e-06\n",
      "Epoch 163/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330477502464.0000 - mae: 312357.1562 - mse: 472111448064.0000\n",
      "Epoch 163: loss did not improve from 329865199616.00000\n",
      "26367/26367 [==============================] - 285s 11ms/step - loss: 330477502464.0000 - mae: 312357.1562 - mse: 472111448064.0000 - lr: 3.9063e-06\n",
      "Epoch 164/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329736028160.0000 - mae: 312173.7500 - mse: 471054614528.0000\n",
      "Epoch 164: loss improved from 329865199616.00000 to 329737207808.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 285s 11ms/step - loss: 329737207808.0000 - mae: 312175.9062 - mse: 471056318464.0000 - lr: 3.9063e-06\n",
      "Epoch 165/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329955049472.0000 - mae: 312000.0312 - mse: 471365844992.0000\n",
      "Epoch 165: loss did not improve from 329737207808.00000\n",
      "26367/26367 [==============================] - 298s 11ms/step - loss: 329955049472.0000 - mae: 312000.0312 - mse: 471365844992.0000 - lr: 3.9063e-06\n",
      "Epoch 166/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 330020585472.0000 - mae: 312238.6875 - mse: 471456546816.0000\n",
      "Epoch 166: loss did not improve from 329737207808.00000\n",
      "26367/26367 [==============================] - 254s 10ms/step - loss: 330020585472.0000 - mae: 312238.6875 - mse: 471456546816.0000 - lr: 3.9063e-06\n",
      "Epoch 167/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 330150117376.0000 - mae: 312136.6562 - mse: 471646175232.0000\n",
      "Epoch 167: loss did not improve from 329737207808.00000\n",
      "26367/26367 [==============================] - 251s 10ms/step - loss: 330110468096.0000 - mae: 312122.4688 - mse: 471589584896.0000 - lr: 3.9063e-06\n",
      "Epoch 168/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329889546240.0000 - mae: 311956.4062 - mse: 471268884480.0000\n",
      "Epoch 168: loss did not improve from 329737207808.00000\n",
      "26367/26367 [==============================] - 243s 9ms/step - loss: 329887088640.0000 - mae: 311956.3438 - mse: 471265378304.0000 - lr: 3.9063e-06\n",
      "Epoch 169/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329903734784.0000 - mae: 312037.7188 - mse: 471287562240.0000\n",
      "Epoch 169: loss did not improve from 329737207808.00000\n",
      "26367/26367 [==============================] - 243s 9ms/step - loss: 329923100672.0000 - mae: 312043.3438 - mse: 471315185664.0000 - lr: 3.9063e-06\n",
      "Epoch 170/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 329605087232.0000 - mae: 312120.5312 - mse: 470866821120.0000\n",
      "Epoch 170: loss improved from 329737207808.00000 to 329605251072.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 241s 9ms/step - loss: 329605251072.0000 - mae: 312120.8750 - mse: 470867050496.0000 - lr: 3.9063e-06\n",
      "Epoch 171/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 329712828416.0000 - mae: 311864.0938 - mse: 471015129088.0000\n",
      "Epoch 171: loss did not improve from 329605251072.00000\n",
      "26367/26367 [==============================] - 240s 9ms/step - loss: 329714008064.0000 - mae: 311865.3750 - mse: 471016800256.0000 - lr: 3.9063e-06\n",
      "Epoch 172/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329811984384.0000 - mae: 312018.6250 - mse: 471161536512.0000\n",
      "Epoch 172: loss did not improve from 329605251072.00000\n",
      "26367/26367 [==============================] - 242s 9ms/step - loss: 329809395712.0000 - mae: 312018.3125 - mse: 471157866496.0000 - lr: 3.9063e-06\n",
      "Epoch 173/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329891577856.0000 - mae: 312097.2812 - mse: 471273504768.0000\n",
      "Epoch 173: loss did not improve from 329605251072.00000\n",
      "26367/26367 [==============================] - 244s 9ms/step - loss: 329889382400.0000 - mae: 312098.2188 - mse: 471270293504.0000 - lr: 3.9063e-06\n",
      "Epoch 174/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329889382400.0000 - mae: 312262.9375 - mse: 471270227968.0000\n",
      "Epoch 174: loss did not improve from 329605251072.00000\n",
      "26367/26367 [==============================] - 260s 10ms/step - loss: 329889382400.0000 - mae: 312262.9375 - mse: 471270227968.0000 - lr: 3.9063e-06\n",
      "Epoch 175/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 329782820864.0000 - mae: 312032.9375 - mse: 471120838656.0000\n",
      "Epoch 175: loss did not improve from 329605251072.00000\n",
      "26367/26367 [==============================] - 254s 10ms/step - loss: 329769320448.0000 - mae: 312033.1562 - mse: 471101603840.0000 - lr: 3.9063e-06\n",
      "Epoch 176/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 329573072896.0000 - mae: 312228.9062 - mse: 470817112064.0000\n",
      "Epoch 176: loss improved from 329605251072.00000 to 329580019712.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 239s 9ms/step - loss: 329580019712.0000 - mae: 312229.8125 - mse: 470826975232.0000 - lr: 3.9063e-06\n",
      "Epoch 177/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 329960488960.0000 - mae: 312181.9688 - mse: 471371677696.0000\n",
      "Epoch 177: loss did not improve from 329580019712.00000\n",
      "26367/26367 [==============================] - 253s 10ms/step - loss: 329949347840.0000 - mae: 312180.9062 - mse: 471355752448.0000 - lr: 3.9063e-06\n",
      "Epoch 178/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 330100473856.0000 - mae: 312230.4375 - mse: 471573954560.0000\n",
      "Epoch 178: loss did not improve from 329580019712.00000\n",
      "26367/26367 [==============================] - 251s 10ms/step - loss: 330088972288.0000 - mae: 312226.5938 - mse: 471557570560.0000 - lr: 3.9063e-06\n",
      "Epoch 179/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 329952165888.0000 - mae: 312005.1875 - mse: 471362142208.0000\n",
      "Epoch 179: loss did not improve from 329580019712.00000\n",
      "26367/26367 [==============================] - 242s 9ms/step - loss: 329930473472.0000 - mae: 311999.9375 - mse: 471331176448.0000 - lr: 3.9063e-06\n",
      "Epoch 180/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329760997376.0000 - mae: 312059.7812 - mse: 471087742976.0000\n",
      "Epoch 180: loss did not improve from 329580019712.00000\n",
      "26367/26367 [==============================] - 243s 9ms/step - loss: 329757720576.0000 - mae: 312060.1250 - mse: 471083057152.0000 - lr: 1.9531e-06\n",
      "Epoch 181/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 330028613632.0000 - mae: 312100.3750 - mse: 471471423488.0000\n",
      "Epoch 181: loss did not improve from 329580019712.00000\n",
      "26367/26367 [==============================] - 239s 9ms/step - loss: 330027696128.0000 - mae: 312101.0000 - mse: 471470145536.0000 - lr: 1.9531e-06\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26366/26367 [============================>.] - ETA: 0s - loss: 329550495744.0000 - mae: 312014.9062 - mse: 470786605056.0000\n",
      "Epoch 182: loss improved from 329580019712.00000 to 329555443712.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 255s 10ms/step - loss: 329555443712.0000 - mae: 312017.1562 - mse: 470793682944.0000 - lr: 1.9531e-06\n",
      "Epoch 183/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329805627392.0000 - mae: 312179.8438 - mse: 471151312896.0000\n",
      "Epoch 183: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 275s 10ms/step - loss: 329805627392.0000 - mae: 312179.8438 - mse: 471151312896.0000 - lr: 1.9531e-06\n",
      "Epoch 184/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329963143168.0000 - mae: 312204.0938 - mse: 471375249408.0000\n",
      "Epoch 184: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 418s 16ms/step - loss: 329963143168.0000 - mae: 312204.0938 - mse: 471375249408.0000 - lr: 1.9531e-06\n",
      "Epoch 185/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329801498624.0000 - mae: 312085.1875 - mse: 471144923136.0000\n",
      "Epoch 185: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 275s 10ms/step - loss: 329801498624.0000 - mae: 312085.1875 - mse: 471144923136.0000 - lr: 1.9531e-06\n",
      "Epoch 186/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329786425344.0000 - mae: 312045.6562 - mse: 471123329024.0000\n",
      "Epoch 186: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 269s 10ms/step - loss: 329786425344.0000 - mae: 312045.6562 - mse: 471123329024.0000 - lr: 1.9531e-06\n",
      "Epoch 187/200\n",
      "26367/26367 [==============================] - ETA: 0s - loss: 329938206720.0000 - mae: 312178.5312 - mse: 471339171840.0000\n",
      "Epoch 187: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 281s 11ms/step - loss: 329938206720.0000 - mae: 312178.5312 - mse: 471339171840.0000 - lr: 1.9531e-06\n",
      "Epoch 188/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329763487744.0000 - mae: 312103.9375 - mse: 471094034432.0000\n",
      "Epoch 188: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 283s 11ms/step - loss: 329760014336.0000 - mae: 312103.6562 - mse: 471089086464.0000 - lr: 1.9531e-06\n",
      "Epoch 189/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329719283712.0000 - mae: 311982.1250 - mse: 471031054336.0000\n",
      "Epoch 189: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 295s 11ms/step - loss: 329714761728.0000 - mae: 311980.0625 - mse: 471024566272.0000 - lr: 1.9531e-06\n",
      "Epoch 190/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 329798680576.0000 - mae: 311932.9375 - mse: 471139221504.0000\n",
      "Epoch 190: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 245s 9ms/step - loss: 329789210624.0000 - mae: 311929.6562 - mse: 471125688320.0000 - lr: 1.9531e-06\n",
      "Epoch 191/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 329626320896.0000 - mae: 311839.0625 - mse: 470896640000.0000\n",
      "Epoch 191: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 265s 10ms/step - loss: 329616359424.0000 - mae: 311839.0625 - mse: 470882418688.0000 - lr: 1.9531e-06\n",
      "Epoch 192/200\n",
      "26364/26367 [============================>.] - ETA: 0s - loss: 329749692416.0000 - mae: 311969.0000 - mse: 471071162368.0000\n",
      "Epoch 192: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 257s 10ms/step - loss: 329730228224.0000 - mae: 311964.3750 - mse: 471043342336.0000 - lr: 1.9531e-06\n",
      "Epoch 193/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329936404480.0000 - mae: 312100.7188 - mse: 471337074688.0000\n",
      "Epoch 193: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 247s 9ms/step - loss: 329932701696.0000 - mae: 312099.4688 - mse: 471331799040.0000 - lr: 1.9531e-06\n",
      "Epoch 194/200\n",
      "26365/26367 [============================>.] - ETA: 0s - loss: 329947119616.0000 - mae: 312103.1250 - mse: 471353589760.0000\n",
      "Epoch 194: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 243s 9ms/step - loss: 329937256448.0000 - mae: 312101.5000 - mse: 471339532288.0000 - lr: 1.9531e-06\n",
      "Epoch 195/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 329603317760.0000 - mae: 311784.5000 - mse: 470865575936.0000\n",
      "Epoch 195: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 241s 9ms/step - loss: 329589424128.0000 - mae: 311781.6562 - mse: 470845685760.0000 - lr: 1.9531e-06\n",
      "Epoch 196/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 329842262016.0000 - mae: 311957.5000 - mse: 471201480704.0000\n",
      "Epoch 196: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 241s 9ms/step - loss: 329815392256.0000 - mae: 311951.0625 - mse: 471163076608.0000 - lr: 1.9531e-06\n",
      "Epoch 197/200\n",
      "26363/26367 [============================>.] - ETA: 0s - loss: 329701720064.0000 - mae: 311916.6250 - mse: 471003234304.0000\n",
      "Epoch 197: loss did not improve from 329555443712.00000\n",
      "26367/26367 [==============================] - 243s 9ms/step - loss: 329734750208.0000 - mae: 311928.4375 - mse: 471050485760.0000 - lr: 1.9531e-06\n",
      "Epoch 198/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329487843328.0000 - mae: 311983.8125 - mse: 470693576704.0000\n",
      "Epoch 198: loss improved from 329555443712.00000 to 329488990208.00000, saving model to best_model.h5\n",
      "26367/26367 [==============================] - 240s 9ms/step - loss: 329488990208.0000 - mae: 311985.7812 - mse: 470695215104.0000 - lr: 1.9531e-06\n",
      "Epoch 199/200\n",
      "26362/26367 [============================>.] - ETA: 0s - loss: 329772630016.0000 - mae: 311988.7500 - mse: 471103700992.0000\n",
      "Epoch 199: loss did not improve from 329488990208.00000\n",
      "26367/26367 [==============================] - 236s 9ms/step - loss: 329742712832.0000 - mae: 311985.5312 - mse: 471060971520.0000 - lr: 1.9531e-06\n",
      "Epoch 200/200\n",
      "26366/26367 [============================>.] - ETA: 0s - loss: 329675669504.0000 - mae: 311931.9062 - mse: 470964273152.0000\n",
      "Epoch 200: loss did not improve from 329488990208.00000\n",
      "26367/26367 [==============================] - 239s 9ms/step - loss: 329699688448.0000 - mae: 311936.5938 - mse: 470998548480.0000 - lr: 9.7656e-07\n",
      "   1/2930 [..............................] - ETA: 29:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:18:43.271463: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2930/2930 [==============================] - 4s 1ms/step\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "MSE: 820123288909.59\n",
      "MAE: 424959.78\n",
      "RMSE: 905606.59\n",
      "R-squared: 0.1491\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_time_series_split(data_df, train_ratio=0.9):\n",
    "    \"\"\"Prepare data with temporal train/test split\"\"\"\n",
    "    if 'date' in data_df.columns:\n",
    "        data_df = data_df.sort_values('date')\n",
    "    \n",
    "    # Calculate split index\n",
    "    n = len(data_df)\n",
    "    train_idx = int(n * train_ratio)\n",
    "    \n",
    "    # Split data temporally\n",
    "    train_df = data_df.iloc[:train_idx]\n",
    "    test_df = data_df.iloc[train_idx:]\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_columns = [col for col in data_df.columns if col != 'sale_price']\n",
    "    \n",
    "    X_train = train_df[X_columns]\n",
    "    X_test = test_df[X_columns]\n",
    "    \n",
    "    y_train = train_df['sale_price'].values\n",
    "    y_test = test_df['sale_price'].values\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_enhanced_model(input_shape, learning_rate=0.001):\n",
    "    \"\"\"Create enhanced model with advanced architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer with regularization\n",
    "        Dense(256, input_shape=input_shape,\n",
    "              kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "              bias_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(128, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(32, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "        mae = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "        return 0.7 * mse + 0.3 * mae\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=custom_loss,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_enhanced_model(model, X_train, y_train, batch_size=32, epochs=200):\n",
    "    \"\"\"Train model without validation split\"\"\"\n",
    "    \n",
    "    def lr_schedule(epoch):\n",
    "        initial_lr = 0.001\n",
    "        drop = 0.5\n",
    "        epochs_drop = 20.0\n",
    "        lr = initial_lr * np.power(drop, np.floor((1 + epoch)/epochs_drop))\n",
    "        return lr\n",
    "    \n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            monitor='loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        lr_scheduler\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Data augmentation for training\n",
    "        augmented_X = X_train.copy()\n",
    "        augmented_y = y_train.copy()\n",
    "        \n",
    "        # Add slight noise to features\n",
    "        noise = np.random.normal(0, 0.001, augmented_X.shape)\n",
    "        augmented_X = augmented_X + noise\n",
    "        \n",
    "        history = model.fit(\n",
    "            augmented_X, augmented_y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model with comprehensive metrics\"\"\"\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mse = np.mean((y_test - y_pred.flatten()) ** 2)\n",
    "        mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        ss_res = np.sum((y_test - y_pred.flatten()) ** 2)\n",
    "        ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        print(\"\\nModel Evaluation Metrics:\")\n",
    "        print(f\"MSE: {mse:.2f}\")\n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "        print(f\"R-squared: {r2:.4f}\")\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Prepare data with 90/10 split\n",
    "        X_train, X_test, y_train, y_test = prepare_time_series_split(data_df)\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_enhanced_model(input_shape=(X_train.shape[1],))\n",
    "        history = train_enhanced_model(model, X_train, y_train)\n",
    "        \n",
    "        if history is not None:\n",
    "            y_pred = evaluate_model(model, X_test, y_test)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a76aef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 424959.7791\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print RMSE\n",
    "rmse = mean_absolute_error(y_test, y_pred.flatten())\n",
    "print(f\"MAE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1933ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
